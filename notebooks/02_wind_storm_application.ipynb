{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: European Winterstorm Return Period Map\n",
    "\n",
    "\n",
    "In this notebook, we download the reanalysis data for European windstorm footprints and build the return period map by fitting a generaliezd Pareto distribution at each pixel. We use dask to fit the distributions in parallel, and apply some simple logic to clean up the data and to select the high thresholds above which the GPD can be considered valid.\n",
    "\n",
    "To set up the Copernicus CDSAPI, see instructions [here](https://cds.climate.copernicus.eu/how-to-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Union, Tuple, Optional\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests import HTTPError\n",
    "\n",
    "import cdsapi\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "from caseva.models import ThresholdExcessModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT = Path().resolve()\n",
    "PATH_DATA = PATH_ROOT / \"data\"\n",
    "PATH_DATA.mkdir(exist_ok=True)\n",
    "\n",
    "COPERNICUS_DATASET = \"sis-european-wind-storm-indicators\"\n",
    "COPERNICUS_PRODUCT = \"windstorm_footprints\"\n",
    "\n",
    "LAT_CHNKS = 152   # The chunk size should divide the data dimensions evenly \n",
    "LON_CHNKS = 232\n",
    "EVENT_CHNKS = -1  # No chunking over the event/time dimension\n",
    "\n",
    "RETURN_PERIODS = np.array([10, 50, 100], dtype=float)\n",
    "\n",
    "# Workers for fitting the distributions in parallel\n",
    "N_WORKERS = 8\n",
    "THREADS_PER_WORKER = 1\n",
    "\n",
    "# Intermediate results and output names.\n",
    "OUTPUT_VARNAME = \"wind_footprint\"\n",
    "EVENT_SET_FILENAME = \"wind_footprints.zarr\"\n",
    "RP_MAP_FILENAME = \"return_period_map.zarr\"\n",
    "\n",
    "# Values above this value are considered erroneous\n",
    "MAX_WIND_SPEED_MPS = 150\n",
    "\n",
    "# If a pixel has fewer data points than this, the fitting is skipped.\n",
    "MIN_NUM_DATA_TO_FIT = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(num: int) -> str:\n",
    "    \"\"\"Integer formatting compatible with the API request syntax.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    pad(2) -> \"02\"\n",
    "    pad(10) -> \"10\"\n",
    "    \"\"\"\n",
    "    return f\"{num:02d}\"\n",
    "\n",
    "# The 9th day of the month is missing from the dataset (= no events).\n",
    "days = [pad(i) for i in range(1, 32) if  i != 9]\n",
    "winter_months = [pad(i) for i in [1, 2, 3, 10, 11 , 12]]\n",
    "\n",
    "# Some years have no data records (= no events occurring)\n",
    "missing_yrs = [2003, 2004, 2010, 2018, 2019]\n",
    "years = [str(yr) for yr in range(1979, 2022) if yr not in missing_yrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_files(\n",
    "    years: List[str],\n",
    "    months: List[str],\n",
    "    days: List[str],\n",
    "    save_dir: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"Fetch wind footprint data files from the Copernicus API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    years : list of str\n",
    "        List of years to fetch.\n",
    "    months : list of str\n",
    "        List of months to fetch.\n",
    "    days : list of str\n",
    "        List of days to fetch.\n",
    "    save_dir : str | Path\n",
    "        Directory for storing the temporary raw data files.\n",
    "    \"\"\"\n",
    "    client = cdsapi.Client()\n",
    "\n",
    "    for year in years:\n",
    "\n",
    "        print(f\"Processing year {year}...\")\n",
    "        for month in months:\n",
    "\n",
    "            request = {\n",
    "                \"product\": [COPERNICUS_PRODUCT],\n",
    "                \"variable\": \"all\",\n",
    "                \"year\": [year],\n",
    "                \"month\": [month],\n",
    "                \"day\": days,\n",
    "            }\n",
    "\n",
    "            target = os.path.join(save_dir, f\"{year}_{month}.zip\")\n",
    "\n",
    "            try:\n",
    "                client.retrieve(COPERNICUS_DATASET, request, target)\n",
    "            except HTTPError:\n",
    "                print(f\"Download failed for {target}. Likely missing data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_timestamp(fname: str) -> pd.Timestamp:\n",
    "    \"\"\"Extract timestamp from Copernicus filename.\n",
    "\n",
    "    The Copernicus files have an 8-digit sequence for YYYMMDD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Name of the netcdf file to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Timestamp\n",
    "        Date corresponding to the wind data of that file.\n",
    "    \"\"\"\n",
    "    time_str = re.search(r'(\\d{8})', fname).group(1)\n",
    "    return pd.to_datetime(time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_spatial_dims(ds: Union[xr.Dataset, xr.DataArray]) -> Tuple[str, str]:\n",
    "    \"\"\"Infer spatial dimension names from xarray data from fixed candidates.\n",
    "    \"\"\"\n",
    "\n",
    "    lon_candidates = [\"longitude\", \"lon\", \"lng\", \"x\"]\n",
    "    lat_candidates = [\"latitude\", \"lat\", \"y\"]\n",
    "    \n",
    "    lower_dims = {dim.lower(): dim for dim in ds.dims}\n",
    "\n",
    "    lon_dims = [lower_dims[dim] for dim in lower_dims if dim in lon_candidates]\n",
    "    lat_dims = [lower_dims[dim] for dim in lower_dims if dim in lat_candidates]\n",
    "\n",
    "    if len(lon_dims) != 1 or len(lat_dims) != 1:\n",
    "        raise ValueError(\"Could not find unique lon/lat names.\")\n",
    "    \n",
    "    return (lon_dims[0], lat_dims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Preprocess `ds` before it is read with the xr.open_mfdataset.\n",
    "    \n",
    "    Steps include:\n",
    "    1. Extract time stamp from the input filename.\n",
    "    2. Drop redundant coordinates and data variables, if any.\n",
    "    3. Rename spatial coordinates to \"lat\" and \"lon\".\n",
    "    4. Rename data variable to \"wind_footprint\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse time coordinate stamp from the .nc filename.\n",
    "    filename = ds.encoding['source']\n",
    "    timestamp = _extract_timestamp(filename)\n",
    "\n",
    "    # Redundant coords to drop.\n",
    "    drop_coords = [coord for coord in ds.coords if coord in [\"time\", \"z\"]]\n",
    "    for coord in drop_coords:\n",
    "        ds = ds.drop_vars(coord)\n",
    "\n",
    "    # `varname` is sometimes 'FX', 'max_wind_speed', etc...\n",
    "    varname = list(ds.data_vars)[0]\n",
    "    xname, yname = _infer_spatial_dims(ds)\n",
    "    return (\n",
    "        ds\n",
    "        .squeeze()\n",
    "        .expand_dims(date=[timestamp])\n",
    "        .rename({varname: OUTPUT_VARNAME, xname: \"lon\", yname: \"lat\"})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "\n",
    "    # Load zip files from Copernicus data store.\n",
    "    download_data_files(years, winter_months, days, save_dir=tmpdir)\n",
    "\n",
    "    # Unpack all zipped netcdf files.\n",
    "    zipfiles = glob(os.path.join(tmpdir, \"*.zip\"))\n",
    "    for zfile in zipfiles:\n",
    "        with zipfile.ZipFile(zfile, \"r\") as file_ref:\n",
    "            file_ref.extractall(tmpdir)\n",
    "\n",
    "    # Combine all netcdf files into a single dataset.\n",
    "    ncfiles = glob(os.path.join(tmpdir, \"*.nc\"))\n",
    "\n",
    "    # Note: chunking is done before renaming. Also, chunking is done on a\n",
    "    # file-by-file basis, meaning that the \"event\" dimension (corresponding\n",
    "    # to each netcdf file needs to be chunked separately afterwards).\n",
    "\n",
    "    ds = xr.open_mfdataset(\n",
    "        ncfiles,\n",
    "        preprocess=preprocess,\n",
    "        chunks={\"Latitude\": LAT_CHNKS, \"Longitude\": LON_CHNKS}\n",
    "    )[OUTPUT_VARNAME].chunk({\"event\": EVENT_CHNKS})\n",
    "\n",
    "    # Write to a zarr file. \n",
    "    ds.to_zarr(PATH_DATA / EVENT_SET_FILENAME, compute=True, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First glance at the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(PATH_DATA / EVENT_SET_FILENAME)[OUTPUT_VARNAME]\n",
    "ds = ds.sortby(\"event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.max(\"event\").plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the extreme value distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "A key assumption is that the extreme data are independent. Therefore, to rule out cases where multiple high values are caused by the same storm track, we add a simple time window threshold: If there are multiple entries for any 3-day window, we only keep the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diffs = ds.event.diff(\"event\")\n",
    "is_clustered_event = time_diffs < np.timedelta64(3, \"D\")\n",
    "\n",
    "first_event_mask = ds.event[0].copy(data=False)\n",
    "exclusion_mask = xr.concat([first_event_mask, is_clustered_event], dim=\"event\")\n",
    "\n",
    "ds = ds.where(~exclusion_mask, drop=True)\n",
    "\n",
    "all_years = pd.DatetimeIndex(ds.event).year\n",
    "num_years = all_years.max() - all_years.min() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_resolve_corner_solution(\n",
    "    data: np.ndarray,\n",
    "    init_thresh: float,\n",
    "    num_years: int,\n",
    "    step_size: float = 0.1,\n",
    "    max_attempts: int = 10,\n",
    "    min_data_size: int = 10,\n",
    ") -> Optional[ThresholdExcessModel]:\n",
    "    \"\"\"Re-fit the model by adjusting the threshold upward and then downward\n",
    "    to try avoiding corner solutions.\n",
    "\n",
    "    This is a very ad-hoc way of automated threshold selection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array-like\n",
    "        The data to be fit by the model.\n",
    "    init_thresh : float\n",
    "        The initial threshold value.\n",
    "    num_years : int\n",
    "        Number of years for the model fit.\n",
    "    step_size : float, optional, default=0.1\n",
    "        The fractional increment/decrement applied to the threshold.\n",
    "    max_attempts : int, optional, default=10\n",
    "        The maximum number of attempts in each direction.\n",
    "    min_data_size : int, optional, default=10\n",
    "        The minimum number of data points above threshold required to fit.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : ThresholdExcessModel or None\n",
    "        A fitted model if a corner solution is successfully resolved; \n",
    "        otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    model = ThresholdExcessModel()\n",
    "    \n",
    "    # 1) Try increasing the threshold\n",
    "    threshold = init_thresh\n",
    "    for _ in range(max_attempts):\n",
    "\n",
    "        threshold *= (1 + step_size)\n",
    "\n",
    "        # If too few data points remain.\n",
    "        if data[data > threshold].size < min_data_size:\n",
    "            break\n",
    "\n",
    "        model.fit(data=data, threshold=threshold, num_years=num_years)\n",
    "\n",
    "        if not model.optimizer.is_corner_solution:\n",
    "            return model\n",
    "    \n",
    "    # 2) Try decreasing the threshold\n",
    "    threshold = init_thresh\n",
    "    for _ in range(max_attempts):\n",
    "        threshold *= (1 - step_size)\n",
    "\n",
    "        model.fit(data=data, threshold=threshold, num_years=num_years)\n",
    "\n",
    "        if not model.optimizer.is_corner_solution:\n",
    "            return model\n",
    "    \n",
    "    # Could not resolve the corner solution\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sqrt_n_threshold(data: np.ndarray) -> float:\n",
    "    \"\"\"Get the threshold leading to sqrt(n) values treated as extremes.\"\"\"\n",
    "    k = int(np.ceil(np.sqrt(data.size)))\n",
    "    tail_indices = np.argpartition(data, -k)[-k:]\n",
    "\n",
    "    return data[tail_indices].min() - 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gpd(\n",
    "    data: np.ndarray,\n",
    "    num_years: int,\n",
    "    resolve_corners=True,\n",
    "    debug: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Fit a generalized Pareto distribution to a 1d data array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Data array corresponding to a time series of one pixel on the map.\n",
    "    num_years : int\n",
    "        The number of years the `data` corresponds to.\n",
    "    resolve_corners : True\n",
    "        Whether to try resolving corner solutions of the optimizer (often a \n",
    "        sign of a poor-quality fit) by adjusting the threshold up and down.\n",
    "    debug : bool, default=False\n",
    "        Whether to return a diagnostics plot of the fitting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array containing the requested return periods.\n",
    "    \"\"\"\n",
    "\n",
    "    if np.isnan(data).all():\n",
    "        return np.full_like(RETURN_PERIODS, np.nan)\n",
    "    elif data[data > 0].size < MIN_NUM_DATA_TO_FIT:\n",
    "        return np.zeros_like(RETURN_PERIODS)\n",
    "    \n",
    "    model = ThresholdExcessModel()\n",
    "    threshold = get_sqrt_n_threshold(data)\n",
    "\n",
    "    try:\n",
    "        model.fit(data=data, threshold=threshold, num_years=num_years)\n",
    "\n",
    "        if resolve_corners and model.optimizer.is_corner_solution:\n",
    "\n",
    "            new_model = try_resolve_corner_solution(data, threshold, num_years)\n",
    "\n",
    "            if new_model is not None:\n",
    "                model = new_model\n",
    "                print(\"Resolved a corner solution.\")\n",
    "\n",
    "        if debug:\n",
    "            model.diagnostic_plot()\n",
    "        return model.return_level(RETURN_PERIODS)[\"level\"]\n",
    "    \n",
    "    except ValueError:\n",
    "        return np.full_like(RETURN_PERIODS, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test fit to a single pixel on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ds.sel(lon=10, lat=45, method=\"nearest\").compute()\n",
    "fit_gpd(test_data.values, num_years=num_years, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval_rp(ds: xr.Dataset, num_years: int) -> np.ndarray:\n",
    "    \"\"\"Fit the distribution to a coarse chunked dataset in parallel.\"\"\"\n",
    "\n",
    "    fits = xr.apply_ufunc(\n",
    "        fit_gpd,\n",
    "        ds,\n",
    "        num_years,\n",
    "        input_core_dims=[[\"event\"], []],\n",
    "        output_core_dims=[[\"return_period\"]],\n",
    "        dask=\"parallelized\",  # Process chunks in parallel\n",
    "        vectorize=True,       # Apply the fit to each (lon/lat) within chunk\n",
    "        dask_gufunc_kwargs={\n",
    "            \"output_sizes\": {\"return_period\": len(RETURN_PERIODS)}\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fits = (\n",
    "        fits\n",
    "        .assign_coords({\"return_period\": RETURN_PERIODS})\n",
    "        .rename({\"return_period\": \"event\"})\n",
    "    )\n",
    "\n",
    "    return fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the fitting\n",
    "\n",
    "Set up a small local cluster to fit the distributions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LocalCluster(\n",
    "    n_workers=N_WORKERS, threads_per_worker=THREADS_PER_WORKER\n",
    ") as cluster:\n",
    "    with Client(cluster) as client:\n",
    "\n",
    "        fit_and_eval_rp(ds, num_years).to_zarr(\n",
    "            RP_MAP_FILENAME,\n",
    "            mode=\"w\",\n",
    "            compute=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "\n",
    "An indication of a poor fit is that the return levels are extremely high even for moderate return periods. Therefore, we first identify if there are any such pixels, try to replace them with valid neighbor values, but if none are available, we set the pixel to a NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_repalce_high_vals(chnk):\n",
    "    \"\"\"Process 3d chunks by replacing high values with neighbor vals or NaNs\"\"\"\n",
    "\n",
    "    maxevent = chnk.max(\"event\")\n",
    "\n",
    "    if maxevent.max() < MAX_WIND_SPEED_MPS or np.isnan(maxevent).all():\n",
    "        return chnk\n",
    "\n",
    "    high_vals = (\n",
    "        maxevent\n",
    "        .where(maxevent >= MAX_WIND_SPEED_MPS, drop=True)\n",
    "        .stack(point=(\"lat\", \"lon\"))\n",
    "    )\n",
    "\n",
    "    for point in high_vals:\n",
    "\n",
    "        lon_indx = chnk.get_index(\"lon\").get_loc(point.lon.item())\n",
    "        lat_indx = chnk.get_index(\"lat\").get_loc(point.lat.item())\n",
    "\n",
    "        neighbors = []\n",
    "        # loop over neighbors\n",
    "        for nx in range(max(lon_indx-1, 0), min(lon_indx+2, chnk.lon.size)):\n",
    "            for ny in range(max(lat_indx-1, 0), min(lat_indx+2, chnk.lat.size)):\n",
    "\n",
    "                if nx == ny == 0:\n",
    "                    continue\n",
    "\n",
    "                neighbors.append((nx, ny))\n",
    "\n",
    "        neighbor_replacement_found = False\n",
    "        for nghbr in neighbors:\n",
    "\n",
    "            neighbor_val = chnk.isel(lon=nghbr[0], lat=nghbr[1]).compute()\n",
    "            if 0 < neighbor_val.max() < MAX_WIND_SPEED_MPS:\n",
    "\n",
    "                chnk.loc[{\n",
    "                    \"lat\": point.lat.item(),\n",
    "                    \"lon\": point.lon.item()\n",
    "                }] = neighbor_val\n",
    "\n",
    "                neighbor_replacement_found = True\n",
    "                break\n",
    "\n",
    "        # Set unresolved ones to zero.\n",
    "        if not neighbor_replacement_found:\n",
    "            chnk.loc[{\n",
    "                \"lat\": point.lat.item(),\n",
    "                \"lon\": point.lon.item()\n",
    "            }] = np.nan\n",
    "\n",
    "    return chnk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(RP_MAP_FILENAME)[OUTPUT_VARNAME]\n",
    "\n",
    "with LocalCluster(\n",
    "    n_workers=N_WORKERS, threads_per_worker=THREADS_PER_WORKER\n",
    ") as cluster:\n",
    "    \n",
    "    template = xr.zeros_like(ds)\n",
    "    with Client(cluster) as client:\n",
    "        ds_final = (\n",
    "            xr.map_blocks(maybe_repalce_high_vals, ds, template=template)\n",
    "            .sel(event=100)\n",
    "            .compute()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_final.plot(figsize=(10, 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caseva_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
